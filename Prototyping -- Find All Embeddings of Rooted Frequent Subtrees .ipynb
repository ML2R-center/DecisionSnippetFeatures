{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Embeddings of the Frequent Patterns\n",
    "\n",
    "It should be easy (and fast enough) to find all embeddings of a few frequent patterns in the random forest database.\n",
    "\n",
    "To do this, we need to \n",
    "\n",
    "- [x] select the right patterns (e.g. those of size six)\n",
    "- [x] read my canonical string format in python / transform it to json\n",
    "- [x] read the json database format in python\n",
    "- [ ] write a small script that traverses the random forests and finds all embeddings and outputs them somehow\n",
    "\n",
    "## Select the Right Patterns (e.g. those of size six)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['61 ( leftChild 63 ( leftChild leaf ) ) ( rightChild leaf ) \\n', '63 ( leftChild 0 ( leftChild leaf ) ( rightChild leaf ) ) \\n', '63 ( leftChild 0 ( leftChild leaf ) ) ( rightChild leaf ) \\n', '63 ( rightChild 9 ( leftChild leaf ) ( rightChild leaf ) ) \\n', '63 ( rightChild 0 ( leftChild leaf ) ( rightChild leaf ) ) \\n', '26 ( rightChild 9 ( leftChild leaf ) ( rightChild leaf ) ) \\n', '0 ( leftChild 9 ( leftChild leaf ) ( rightChild leaf ) ) \\n', '0 ( leftChild 9 ( leftChild leaf ) ) ( rightChild leaf ) \\n', '0 ( rightChild 0 ( leftChild leaf ) ( rightChild leaf ) ) \\n', '0 ( rightChild 63 ( leftChild leaf ) ( rightChild leaf ) ) \\n', '0 ( rightChild 9 ( leftChild leaf ) ( rightChild leaf ) ) \\n', '9 ( leftChild leaf ) ( rightChild 0 ( rightChild leaf ) ) \\n', '9 ( rightChild 0 ( leftChild leaf ) ( rightChild leaf ) ) \\n', '9 ( rightChild 9 ( leftChild leaf ) ( rightChild leaf ) ) \\n']\n"
     ]
    }
   ],
   "source": [
    "# pattern size in number of vertices\n",
    "patternSize = 4\n",
    "# here, we count the number of edges in the pattern\n",
    "patternSize = patternSize - 1 \n",
    "\n",
    "filename = 'forests/rootedFrequentTrees/adult/WithLeafEdges/RF_10_t10.patterns'\n",
    "f = open(filename)\n",
    "\n",
    "# gives us the patterns of the selected size\n",
    "frequentPatterns = filter(lambda line: line.count('(') == patternSize, f)\n",
    "\n",
    "# gives us only the canonical strings of the patterns\n",
    "cStrings = map(lambda fp: fp.split('\\t')[2], frequentPatterns)\n",
    "\n",
    "cStringList = list(cStrings)\n",
    "print(cStringList)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform my canonical string format to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\":0,\"feature\":61,\"leftChild\":{\"id\":1,\"feature\":63,\"leftChild\":{\"id\":2,\"prediction\":[]}},\"rightChild\":{\"id\":3,\"prediction\":[]}}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '61 ( leftChild 63 ( leftChild leaf ) ) ( rightChild leaf ) \\n'\n",
    "\n",
    "\n",
    "\n",
    "def cString2json(cString):\n",
    "    '''Pascals canonical string format and the json format used in Dortmund are \n",
    "    basically identical (up to ordering, symbols, and general feeling of course ;) ).\n",
    "    This is a converter that transforms a single tree from cString format to json format \n",
    "    (entirely by string maipulation).'''\n",
    "    \n",
    "    intermediate = cString.replace('( leftChild', ',\"leftChild\":{').replace('( rightChild', ',\"rightChild\":{').replace(')', '}').replace('leaf', '-1 \"prediction\":[]')\n",
    "    tokens = intermediate.split(' ')\n",
    "    \n",
    "    json = ''\n",
    "    i = 0\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            feature = int(t)\n",
    "            if feature != -1:\n",
    "                s = '\"id\":' + str(i) + ',\"feature\":' + t\n",
    "            else:\n",
    "                s = '\"id\":' + str(i) + ','\n",
    "            json += s\n",
    "            i += 1\n",
    "        except ValueError:\n",
    "            json += t\n",
    "            \n",
    "            \n",
    "    return ('{' + json.rstrip() + '}')\n",
    "    \n",
    "cString2json(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"id\":0,\"feature\":61,\"leftChild\":{\"id\":1,\"feature\":63,\"leftChild\":{\"id\":2,\"prediction\":[]}},\"rightChild\":{\"id\":3,\"prediction\":[]}},\n",
      "{\"id\":0,\"feature\":63,\"leftChild\":{\"id\":1,\"feature\":0,\"leftChild\":{\"id\":2,\"prediction\":[]},\"rightChild\":{\"id\":3,\"prediction\":[]}}},\n",
      "{\"id\":0,\"feature\":63,\"leftChild\":{\"id\":1,\"feature\":0,\"leftChild\":{\"id\":2,\"prediction\":[]}},\"rightChild\":{\"id\":3,\"prediction\":[]}},\n",
      "{\"id\":0,\"feature\":63,\"rightChild\":{\"id\":1,\"feature\":9,\"leftChild\":{\"id\":2,\"prediction\":[]},\"rightChild\":{\"id\":3,\"prediction\":[]}}},\n",
      "{\"id\":0,\"feature\":63,\"rightChild\":{\"id\":1,\"feature\":0,\"leftChild\":{\"id\":2,\"prediction\":[]},\"rightChild\":{\"id\":3,\"prediction\":[]}}},\n",
      "{\"id\":0,\"feature\":26,\"rightChild\":{\"id\":1,\"feature\":9,\"leftChild\":{\"id\":2,\"prediction\":[]},\"rightChild\":{\"id\":3,\"prediction\":[]}}},\n",
      "{\"id\":0,\"feature\":0,\"leftChild\":{\"id\":1,\"feature\":9,\"leftChild\":{\"id\":2,\"prediction\":[]},\"rightChild\":{\"id\":3,\"prediction\":[]}}},\n",
      "{\"id\":0,\"feature\":0,\"leftChild\":{\"id\":1,\"feature\":9,\"leftChild\":{\"id\":2,\"prediction\":[]}},\"rightChild\":{\"id\":3,\"prediction\":[]}},\n",
      "{\"id\":0,\"feature\":0,\"rightChild\":{\"id\":1,\"feature\":0,\"leftChild\":{\"id\":2,\"prediction\":[]},\"rightChild\":{\"id\":3,\"prediction\":[]}}},\n",
      "{\"id\":0,\"feature\":0,\"rightChild\":{\"id\":1,\"feature\":63,\"leftChild\":{\"id\":2,\"prediction\":[]},\"rightChild\":{\"id\":3,\"prediction\":[]}}},\n",
      "{\"id\":0,\"feature\":0,\"rightChild\":{\"id\":1,\"feature\":9,\"leftChild\":{\"id\":2,\"prediction\":[]},\"rightChild\":{\"id\":3,\"prediction\":[]}}},\n",
      "{\"id\":0,\"feature\":9,\"leftChild\":{\"id\":1,\"prediction\":[]},\"rightChild\":{\"id\":2,\"feature\":0,\"rightChild\":{\"id\":3,\"prediction\":[]}}},\n",
      "{\"id\":0,\"feature\":9,\"rightChild\":{\"id\":1,\"feature\":0,\"leftChild\":{\"id\":2,\"prediction\":[]},\"rightChild\":{\"id\":3,\"prediction\":[]}}},\n",
      "{\"id\":0,\"feature\":9,\"rightChild\":{\"id\":1,\"feature\":9,\"leftChild\":{\"id\":2,\"prediction\":[]},\"rightChild\":{\"id\":3,\"prediction\":[]}}}]\n"
     ]
    }
   ],
   "source": [
    "jsons = '[' + ',\\n'.join(map(cString2json, cStringList)) + ']'\n",
    "print(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pattern': {'feature': 61,\n",
       "   'id': 0,\n",
       "   'leftChild': {'feature': 63,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []}},\n",
       "   'rightChild': {'id': 3, 'prediction': []}},\n",
       "  'patternid': 339},\n",
       " {'pattern': {'feature': 63,\n",
       "   'id': 0,\n",
       "   'leftChild': {'feature': 0,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []},\n",
       "    'rightChild': {'id': 3, 'prediction': []}}},\n",
       "  'patternid': 337},\n",
       " {'pattern': {'feature': 63,\n",
       "   'id': 0,\n",
       "   'leftChild': {'feature': 0,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []}},\n",
       "   'rightChild': {'id': 3, 'prediction': []}},\n",
       "  'patternid': 336},\n",
       " {'pattern': {'feature': 63,\n",
       "   'id': 0,\n",
       "   'rightChild': {'feature': 9,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []},\n",
       "    'rightChild': {'id': 3, 'prediction': []}}},\n",
       "  'patternid': 338},\n",
       " {'pattern': {'feature': 63,\n",
       "   'id': 0,\n",
       "   'rightChild': {'feature': 0,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []},\n",
       "    'rightChild': {'id': 3, 'prediction': []}}},\n",
       "  'patternid': 335},\n",
       " {'pattern': {'feature': 26,\n",
       "   'id': 0,\n",
       "   'rightChild': {'feature': 9,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []},\n",
       "    'rightChild': {'id': 3, 'prediction': []}}},\n",
       "  'patternid': 334},\n",
       " {'pattern': {'feature': 0,\n",
       "   'id': 0,\n",
       "   'leftChild': {'feature': 9,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []},\n",
       "    'rightChild': {'id': 3, 'prediction': []}}},\n",
       "  'patternid': 333},\n",
       " {'pattern': {'feature': 0,\n",
       "   'id': 0,\n",
       "   'leftChild': {'feature': 9,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []}},\n",
       "   'rightChild': {'id': 3, 'prediction': []}},\n",
       "  'patternid': 332},\n",
       " {'pattern': {'feature': 0,\n",
       "   'id': 0,\n",
       "   'rightChild': {'feature': 0,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []},\n",
       "    'rightChild': {'id': 3, 'prediction': []}}},\n",
       "  'patternid': 331},\n",
       " {'pattern': {'feature': 0,\n",
       "   'id': 0,\n",
       "   'rightChild': {'feature': 63,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []},\n",
       "    'rightChild': {'id': 3, 'prediction': []}}},\n",
       "  'patternid': 330},\n",
       " {'pattern': {'feature': 0,\n",
       "   'id': 0,\n",
       "   'rightChild': {'feature': 9,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []},\n",
       "    'rightChild': {'id': 3, 'prediction': []}}},\n",
       "  'patternid': 329},\n",
       " {'pattern': {'feature': 9,\n",
       "   'id': 0,\n",
       "   'leftChild': {'id': 1, 'prediction': []},\n",
       "   'rightChild': {'feature': 0,\n",
       "    'id': 2,\n",
       "    'rightChild': {'id': 3, 'prediction': []}}},\n",
       "  'patternid': 327},\n",
       " {'pattern': {'feature': 9,\n",
       "   'id': 0,\n",
       "   'rightChild': {'feature': 0,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []},\n",
       "    'rightChild': {'id': 3, 'prediction': []}}},\n",
       "  'patternid': 328},\n",
       " {'pattern': {'feature': 9,\n",
       "   'id': 0,\n",
       "   'rightChild': {'feature': 9,\n",
       "    'id': 1,\n",
       "    'leftChild': {'id': 2, 'prediction': []},\n",
       "    'rightChild': {'id': 3, 'prediction': []}}},\n",
       "  'patternid': 326}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseCStringFile(fileName, patternSize):\n",
    "\t# here, we count the number of edges in the pattern\n",
    "\tpatternSize = patternSize - 1 \n",
    "\n",
    "\tf = open(fileName)\n",
    "\n",
    "\t# gives us the patterns of the selected size\n",
    "\tfrequentPatterns = filter(lambda line: line.count('(') == patternSize, f)\n",
    "\n",
    "\t# splits the strings into fields\n",
    "\ttokens = map(lambda fp: fp.split('\\t'), frequentPatterns)\n",
    "\n",
    "\t# gives us only the canonical strings of the patterns and their id\n",
    "\tpairs = map(lambda t: (t[1], t[2]), tokens)\n",
    "\n",
    "\t# transform to json strings\n",
    "\tjsonCStrings = map(lambda pair: '{\"patternid\":' + pair[0] + ',\"pattern\":' + cString2json(pair[1]) + '}', pairs)\n",
    "\n",
    "\tjsonBlob = '[' + ',\\n'.join(jsonCStrings) + ']'\n",
    "\n",
    "\tf.close()\n",
    "\n",
    "\treturn jsonBlob\n",
    "\n",
    "json.loads(parseCStringFile('forests/rootedFrequentTrees/adult/WithLeafEdges/RF_10_t10.patterns', 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write a small script that traverses the random forests and finds all embeddings and outputs them somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"feature\": 61,\n",
      "        \"id\": 0,\n",
      "        \"leftChild\": {\n",
      "            \"feature\": 63,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        },\n",
      "        \"rightChild\": {\n",
      "            \"id\": 3,\n",
      "            \"prediction\": []\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 63,\n",
      "        \"id\": 0,\n",
      "        \"leftChild\": {\n",
      "            \"feature\": 0,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            },\n",
      "            \"rightChild\": {\n",
      "                \"id\": 3,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 63,\n",
      "        \"id\": 0,\n",
      "        \"leftChild\": {\n",
      "            \"feature\": 0,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        },\n",
      "        \"rightChild\": {\n",
      "            \"id\": 3,\n",
      "            \"prediction\": []\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 63,\n",
      "        \"id\": 0,\n",
      "        \"rightChild\": {\n",
      "            \"feature\": 9,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            },\n",
      "            \"rightChild\": {\n",
      "                \"id\": 3,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 63,\n",
      "        \"id\": 0,\n",
      "        \"rightChild\": {\n",
      "            \"feature\": 0,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            },\n",
      "            \"rightChild\": {\n",
      "                \"id\": 3,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 26,\n",
      "        \"id\": 0,\n",
      "        \"rightChild\": {\n",
      "            \"feature\": 9,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            },\n",
      "            \"rightChild\": {\n",
      "                \"id\": 3,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 0,\n",
      "        \"id\": 0,\n",
      "        \"leftChild\": {\n",
      "            \"feature\": 9,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            },\n",
      "            \"rightChild\": {\n",
      "                \"id\": 3,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 0,\n",
      "        \"id\": 0,\n",
      "        \"leftChild\": {\n",
      "            \"feature\": 9,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        },\n",
      "        \"rightChild\": {\n",
      "            \"id\": 3,\n",
      "            \"prediction\": []\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 0,\n",
      "        \"id\": 0,\n",
      "        \"rightChild\": {\n",
      "            \"feature\": 0,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            },\n",
      "            \"rightChild\": {\n",
      "                \"id\": 3,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 0,\n",
      "        \"id\": 0,\n",
      "        \"rightChild\": {\n",
      "            \"feature\": 63,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            },\n",
      "            \"rightChild\": {\n",
      "                \"id\": 3,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 0,\n",
      "        \"id\": 0,\n",
      "        \"rightChild\": {\n",
      "            \"feature\": 9,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            },\n",
      "            \"rightChild\": {\n",
      "                \"id\": 3,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 9,\n",
      "        \"id\": 0,\n",
      "        \"leftChild\": {\n",
      "            \"id\": 1,\n",
      "            \"prediction\": []\n",
      "        },\n",
      "        \"rightChild\": {\n",
      "            \"feature\": 0,\n",
      "            \"id\": 2,\n",
      "            \"rightChild\": {\n",
      "                \"id\": 3,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 9,\n",
      "        \"id\": 0,\n",
      "        \"rightChild\": {\n",
      "            \"feature\": 0,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            },\n",
      "            \"rightChild\": {\n",
      "                \"id\": 3,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"feature\": 9,\n",
      "        \"id\": 0,\n",
      "        \"rightChild\": {\n",
      "            \"feature\": 9,\n",
      "            \"id\": 1,\n",
      "            \"leftChild\": {\n",
      "                \"id\": 2,\n",
      "                \"prediction\": []\n",
      "            },\n",
      "            \"rightChild\": {\n",
      "                \"id\": 3,\n",
      "                \"prediction\": []\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "j = json.loads(jsons)\n",
    "print(json.dumps(j, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There must be a smarter way than the following. \n",
    "However, this is rather easy to implement:\n",
    "We iterate (recursively) over the transaction (decision) tree vertices $v$ and check whether there is a rooted subgraph isomorphism from pattern to the transaction mapping the root of pattern to $v$.\n",
    "This is decided by (again) recursion over pattern and transaction simultaneously, as long as it fits.\n",
    "\n",
    "We'll see whether this is fast enough for our case. Its something along $O(n * p)$ where $n$ and $p$ are the numbers of vertices of transactions and patterns, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printInfo(pattern, transaction, where):\n",
    "    print(where)\n",
    "    print('p: ' + str(pattern))\n",
    "    print('t: ' + str(transaction))\n",
    "\n",
    "\n",
    "def recSearch(pattern, transaction, mapping):\n",
    "    # check if we are in a leaf vertex in both pattern and transaction\n",
    "    if 'prediction' in pattern.keys() and 'prediction' in transaction.keys():\n",
    "        mapping[pattern['id']] = transaction['id']\n",
    "        return True\n",
    "    \n",
    "    # check if we are in a split vertex in both pattern and transaction\n",
    "    if 'feature' in pattern.keys() and 'feature' in transaction.keys():\n",
    "\n",
    "        # check if split features match\n",
    "        if pattern['feature'] == transaction['feature']:\n",
    "            \n",
    "            foundLeft = True\n",
    "            foundRight = True\n",
    "            if 'leftChild' in pattern.keys():\n",
    "                if 'leftChild' in transaction.keys():\n",
    "                    foundLeft = recSearch(pattern['leftChild'], transaction['leftChild'], mapping)    \n",
    "                else:\n",
    "                    foundLeft = False\n",
    "                \n",
    "            if 'rightChild' in pattern.keys():\n",
    "                if 'rightChild' in transaction.keys():\n",
    "                    foundRight = recSearch(pattern['rightChild'], transaction['rightChild'], mapping)\n",
    "                else:\n",
    "                    foundRight = False\n",
    "                \n",
    "            if foundLeft and foundRight:\n",
    "                mapping[pattern['id']] = transaction['id']\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "    # if we are in the mixed case split vertex vs. leaf vertex then we cannot map the vertices on each other\n",
    "    return False\n",
    "\n",
    "\n",
    "def searchEmbedding(pattern, transaction):\n",
    "    '''For two given root vertices, check whether pattern is a rooted \n",
    "    subtree and return a mapping id->id if so, o/w None'''\n",
    "    \n",
    "    mapping = dict()\n",
    "    if recSearch(pattern, transaction, mapping):\n",
    "        return mapping\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def allEmbeddingsRec(pattern, transaction, result):\n",
    "    print('process transaction vertex id ' + str(transaction['id']))\n",
    "    if 'feature' in transaction.keys():\n",
    "        if 'leftChild' in transaction.keys():\n",
    "            allEmbeddingsRec(pattern, transaction['leftChild'], result)\n",
    "        if 'rightChild' in transaction.keys():\n",
    "            allEmbeddingsRec(pattern, transaction['rightChild'], result)\n",
    "    \n",
    "    result.append((transaction['id'], searchEmbedding(pattern, transaction)))\n",
    "    return result\n",
    "\n",
    "\n",
    "def allEmbeddings(pattern, transaction):\n",
    "    embeddingsAndNone = allEmbeddingsRec(pattern, transaction, list())\n",
    "    return embeddingsAndNone #list(filter(lambda x: x != None, embeddingsAndNone))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (1, None), (3, None), (0, {2: 2, 1: 1, 3: 3, 0: 0})]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (3, None), (1, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (1, None), (3, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (3, None), (1, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (3, None), (1, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (3, None), (1, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (3, None), (1, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (1, None), (3, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (3, None), (1, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (3, None), (1, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (3, None), (1, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(1, None), (3, None), (2, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (3, None), (1, None), (0, None)]\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, None), (3, None), (1, None), (0, None)]\n"
     ]
    }
   ],
   "source": [
    "for transaction in j:\n",
    "    print(allEmbeddings(j[0], transaction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'feature': 61, 'leftChild': {'id': 1, 'feature': 63, 'leftChild': {'id': 2, 'prediction': []}}, 'rightChild': {'id': 3, 'prediction': []}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (1, None), (3, {1: 3}), (0, None)]\n",
      "{'id': 0, 'feature': 63, 'leftChild': {'id': 1, 'feature': 0, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (3, {1: 3}), (1, None), (0, None)]\n",
      "{'id': 0, 'feature': 63, 'leftChild': {'id': 1, 'feature': 0, 'leftChild': {'id': 2, 'prediction': []}}, 'rightChild': {'id': 3, 'prediction': []}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (1, None), (3, {1: 3}), (0, None)]\n",
      "{'id': 0, 'feature': 63, 'rightChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (3, {1: 3}), (1, None), (0, None)]\n",
      "{'id': 0, 'feature': 63, 'rightChild': {'id': 1, 'feature': 0, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (3, {1: 3}), (1, None), (0, None)]\n",
      "{'id': 0, 'feature': 26, 'rightChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (3, {1: 3}), (1, None), (0, None)]\n",
      "{'id': 0, 'feature': 0, 'leftChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (3, {1: 3}), (1, None), (0, None)]\n",
      "{'id': 0, 'feature': 0, 'leftChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}}, 'rightChild': {'id': 3, 'prediction': []}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (1, None), (3, {1: 3}), (0, None)]\n",
      "{'id': 0, 'feature': 0, 'rightChild': {'id': 1, 'feature': 0, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (3, {1: 3}), (1, None), (0, None)]\n",
      "{'id': 0, 'feature': 0, 'rightChild': {'id': 1, 'feature': 63, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (3, {1: 3}), (1, None), (0, None)]\n",
      "{'id': 0, 'feature': 0, 'rightChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (3, {1: 3}), (1, None), (0, None)]\n",
      "{'id': 0, 'feature': 9, 'leftChild': {'id': 1, 'prediction': []}, 'rightChild': {'id': 2, 'feature': 0, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(1, {1: 1}), (3, {1: 3}), (2, None), (0, None)]\n",
      "{'id': 0, 'feature': 9, 'rightChild': {'id': 1, 'feature': 0, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (3, {1: 3}), (1, None), (0, None)]\n",
      "{'id': 0, 'feature': 9, 'rightChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "process transaction vertex id 0\n",
      "process transaction vertex id 1\n",
      "process transaction vertex id 2\n",
      "process transaction vertex id 3\n",
      "[(2, {1: 2}), (3, {1: 3}), (1, None), (0, None)]\n"
     ]
    }
   ],
   "source": [
    "leaf = json.loads('{\"id\": 1,\"prediction\": []}')\n",
    "for transaction in j:\n",
    "    print(transaction)\n",
    "    print(allEmbeddings(leaf, transaction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script, without Debug Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printInfo(pattern, transaction, where):\n",
    "    print(where)\n",
    "    print('p: ' + str(pattern))\n",
    "    print('t: ' + str(transaction))\n",
    "\n",
    "\n",
    "def recSearch(pattern, transaction, mapping):\n",
    "    # check if we are in a leaf vertex in both pattern and transaction\n",
    "    if 'prediction' in pattern.keys() and 'prediction' in transaction.keys():\n",
    "        mapping[pattern['id']] = transaction['id']\n",
    "        return True\n",
    "    \n",
    "    # check if we are in a split vertex in both pattern and transaction\n",
    "    if 'feature' in pattern.keys() and 'feature' in transaction.keys():\n",
    "\n",
    "        # check if split features match\n",
    "        if pattern['feature'] == transaction['feature']:\n",
    "            \n",
    "            foundLeft = True\n",
    "            foundRight = True\n",
    "            if 'leftChild' in pattern.keys():\n",
    "                if 'leftChild' in transaction.keys():\n",
    "                    foundLeft = recSearch(pattern['leftChild'], transaction['leftChild'], mapping)    \n",
    "                else:\n",
    "                    foundLeft = False\n",
    "                \n",
    "            if 'rightChild' in pattern.keys():\n",
    "                if 'rightChild' in transaction.keys():\n",
    "                    foundRight = recSearch(pattern['rightChild'], transaction['rightChild'], mapping)\n",
    "                else:\n",
    "                    foundRight = False\n",
    "                \n",
    "            if foundLeft and foundRight:\n",
    "                mapping[pattern['id']] = transaction['id']\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "    # if we are in the mixed case split vertex vs. leaf vertex then we cannot map the vertices on each other\n",
    "    return False\n",
    "\n",
    "\n",
    "def searchEmbedding(pattern, transaction):\n",
    "    '''For two given root vertices, check whether pattern is a rooted \n",
    "    subtree and return a mapping id->id if so, o/w None'''\n",
    "    \n",
    "    mapping = dict()\n",
    "    if recSearch(pattern, transaction, mapping):\n",
    "        return mapping\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def allEmbeddingsRec(pattern, transaction, result):\n",
    "    if 'feature' in transaction.keys():\n",
    "        if 'leftChild' in transaction.keys():\n",
    "            allEmbeddingsRec(pattern, transaction['leftChild'], result)\n",
    "        if 'rightChild' in transaction.keys():\n",
    "            allEmbeddingsRec(pattern, transaction['rightChild'], result)\n",
    "    \n",
    "    result.append(searchEmbedding(pattern, transaction))\n",
    "    return result\n",
    "\n",
    "\n",
    "def allEmbeddings(pattern, transaction):\n",
    "    embeddingsAndNone = allEmbeddingsRec(pattern, transaction, list())\n",
    "    return list(filter(lambda x: x != None, embeddingsAndNone))\n",
    "\n",
    "\n",
    "\n",
    "# def main(file, out):\n",
    "#     f = open(file)\n",
    "#     j = json.load(f)\n",
    "#     f.close()\n",
    "\n",
    "#     graphCounter = 0\n",
    "#     for tree in j:\n",
    "#         vertexLabels, edges = parseTree(tree)\n",
    "#         transform2GraphDB(vertexLabels, edges, graphCounter, out)\n",
    "#         graphCounter += 1\n",
    "#     print('$')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main(sys.argv[1], sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'feature': 61, 'leftChild': {'id': 1, 'feature': 63, 'leftChild': {'id': 2, 'prediction': []}}, 'rightChild': {'id': 3, 'prediction': []}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 63, 'leftChild': {'id': 1, 'feature': 0, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 63, 'leftChild': {'id': 1, 'feature': 0, 'leftChild': {'id': 2, 'prediction': []}}, 'rightChild': {'id': 3, 'prediction': []}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 63, 'rightChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 63, 'rightChild': {'id': 1, 'feature': 0, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 26, 'rightChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 0, 'leftChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 0, 'leftChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}}, 'rightChild': {'id': 3, 'prediction': []}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 0, 'rightChild': {'id': 1, 'feature': 0, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 0, 'rightChild': {'id': 1, 'feature': 63, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 0, 'rightChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 9, 'leftChild': {'id': 1, 'prediction': []}, 'rightChild': {'id': 2, 'feature': 0, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "[{1: 1}, {1: 3}]\n",
      "{'id': 0, 'feature': 9, 'rightChild': {'id': 1, 'feature': 0, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "[{1: 2}, {1: 3}]\n",
      "{'id': 0, 'feature': 9, 'rightChild': {'id': 1, 'feature': 9, 'leftChild': {'id': 2, 'prediction': []}, 'rightChild': {'id': 3, 'prediction': []}}}\n",
      "[{1: 2}, {1: 3}]\n"
     ]
    }
   ],
   "source": [
    "leaf = json.loads('{\"id\": 1,\"prediction\": []}')\n",
    "for transaction in j:\n",
    "    print(transaction)\n",
    "    print(allEmbeddings(leaf, transaction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 17, 10, 18, 15, 12, 20, 10, 14, 12, 14, 13, 16, 23]\n"
     ]
    }
   ],
   "source": [
    "filename = 'forests/adult/text/RF_10.json'\n",
    "f = open(filename, 'r')\n",
    "transactions = json.load(f)\n",
    "f.close()\n",
    "\n",
    "patterns = j\n",
    "\n",
    "embeddingCounts = list()\n",
    "\n",
    "for pattern in patterns:\n",
    "    counts = 0\n",
    "    for transaction in transactions:\n",
    "        mappings = allEmbeddings(pattern, transaction)\n",
    "        counts += len(mappings)\n",
    "    embeddingCounts.append(counts)\n",
    "print(embeddingCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'feature': 61, 'leftChild': {'id': 1, 'feature': 63, 'leftChild': {'id': 2, 'prediction': []}}, 'rightChild': {'id': 3, 'prediction': []}}\n"
     ]
    }
   ],
   "source": [
    "print(patterns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 205, 1: 206, 2: 207, 3: 211}, {0: 620, 1: 621, 2: 622, 3: 628}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allEmbeddings(patterns[0], transactions[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 14, 10, 15, 11, 11, 15, 10, 10, 10, 11, 10, 10, 13]\n"
     ]
    }
   ],
   "source": [
    "filename = 'forests/rootedFrequentTrees/adult/WithLeafEdges/RF_10_t10.patterns'\n",
    "f = open(filename)\n",
    "\n",
    "# gives us the patterns of the selected size\n",
    "frequentPatterns = filter(lambda line: line.count('(') == patternSize, f)\n",
    "\n",
    "# gives us only the canonical strings of the patterns\n",
    "patternCountsFromAlg = list(map(lambda fp: int(fp.split('\\t')[0]), frequentPatterns))\n",
    "\n",
    "print(patternCountsFromAlg)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to output to embeddings that are found in the transaction forest\n",
    "\n",
    "I have written cString2json.py that converts from my format to some json format that contains an object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "'''Transform the canonical string format that is given by the lwg and lwgr \n",
    "programs to a json format that is compatible to the format of Sebastian.\n",
    "reads from stdin and prints to stdout\n",
    "\n",
    "usage: cString2json.py leq|eq patternSize < patternFile > jsonFile\n",
    "\n",
    "leq results in all patterns up to patternSize vertices being converted,\n",
    "eq results in all patterns of exactly patternSize vertices being converted.'''\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def cString2json(cString):\n",
    "\t'''Pascals canonical string format and the json format used in Dortmund are \n",
    "\tbasically identical (up to ordering, symbols, and general feeling of course ;) ).\n",
    "\tThis is a converter that transforms a single tree from cString format to json format \n",
    "\t(entirely by string maipulation).'''\n",
    "\t\n",
    "\tintermediate = cString.replace('( leftChild', ',\"leftChild\":{').replace('( rightChild', ',\"rightChild\":{').replace(')', '}').replace('leaf', '-1 \"prediction\":[]')\n",
    "\ttokens = intermediate.split(' ')\n",
    "\t\n",
    "\tjson = ''\n",
    "\ti = 0\n",
    "\tfor t in tokens:\n",
    "\t\ttry:\n",
    "\t\t\tfeature = int(t)\n",
    "\t\t\tif feature != -1:\n",
    "\t\t\t    s = '\"id\":' + str(i) + ',\"feature\":' + t\n",
    "\t\t\telse:\n",
    "\t\t\t    s = '\"id\":' + str(i) + ','\n",
    "\t\t\tjson += s\n",
    "\t\t\ti += 1\n",
    "\t\texcept ValueError:\n",
    "\t\t\tjson += t\n",
    "\t\t    \n",
    "\t\t    \n",
    "\treturn ('{' + json.rstrip() + '}')\n",
    "\n",
    "\n",
    "def parseCStringFileFixedSizePatterns(fIn,  patternSize):\n",
    "\t'''Select the patterns with patternSize vertices from the file f\n",
    "\twith filename. f is assumed to be in the format that lwg or lwgr \n",
    "\tuses to store the frequent patterns.'''\n",
    "\n",
    "\t# here, we count the number of edges in the pattern\n",
    "\tpatternSize = patternSize - 1 \n",
    "\n",
    "\t# gives us the patterns of the selected size\n",
    "\tfrequentPatterns = filter(lambda line: line.count('(') == patternSize, fIn)\n",
    "\n",
    "\t# splits the strings into fields\n",
    "\ttokens = map(lambda fp: fp.split('\\t'), frequentPatterns)\n",
    "\n",
    "\t# gives us only the canonical strings of the patterns and their id\n",
    "\tpairs = map(lambda t: (t[1], t[2]), tokens)\n",
    "\n",
    "\t# transform to json strings\n",
    "\tjsonCStrings = map(lambda pair: '{\"patternid\":' + pair[0] + ',\"pattern\":' + cString2json(pair[1]) + '}', pairs)\n",
    "\n",
    "\t# if your memory explodes, feel free to change this line and the output mode of this function\n",
    "\tjsonBlob = '[' + ',\\n'.join(jsonCStrings) + ']'\n",
    "\n",
    "\treturn jsonBlob\n",
    "\n",
    "\n",
    "def parseCStringFileUpToSizePatterns(fIn, patternSize):\n",
    "\t'''Select the patterns up to patternSize vertices from the file f\n",
    "\twith filename. f is assumed to be in the format that lwg or lwgr \n",
    "\tuses to store the frequent patterns.'''\n",
    "\n",
    "\t# here, we count the number of edges in the pattern\n",
    "\tpatternSize = patternSize - 1 \n",
    "\n",
    "\t# gives us the patterns of the selected size\n",
    "\tfrequentPatterns = filter(lambda line: line.count('(') <= patternSize, fIn)\n",
    "\n",
    "\t# splits the strings into fields\n",
    "\ttokens = map(lambda fp: fp.split('\\t'), frequentPatterns)\n",
    "\n",
    "\t# gives us only the canonical strings of the patterns and their id\n",
    "\tpairs = map(lambda t: (t[1], t[2]), tokens)\n",
    "\n",
    "\t# transform to json strings\n",
    "\tjsonCStrings = map(lambda pair: '{\"patternid\":' + pair[0] + ',\"pattern\":' + cString2json(pair[1]) + '}', pairs)\n",
    "\n",
    "\t# if your memory explodes, feel free to change this line and the output mode of this function\n",
    "\tjsonBlob = '[' + ',\\n'.join(jsonCStrings) + ']'\n",
    "\n",
    "\treturn jsonBlob\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "# \tif len(sys.argv) != 3:\n",
    "# \t\tsys.stderr.write('You need exactly two arguments: first leq or eq, second an integer.\\n')\n",
    "# \t\tsys.exit(1)\n",
    "# \telse:\n",
    "# \t\ttry:\n",
    "# \t\t\tknownFlag = False\n",
    "# \t\t\tif sys.argv[1] == 'leq': \n",
    "# \t\t\t\tresult = parseCStringFileUpToSizePatterns(sys.stdin, int(sys.argv[2]))\n",
    "# \t\t\t\tknownFlag = True\n",
    "# \t\t\tif sys.argv[1] == 'eq': \n",
    "# \t\t\t\tresult = parseCStringFileFixedSizePatterns(sys.stdin, int(sys.argv[2]))\n",
    "# \t\t\t\tknownFlag = True\n",
    "\t\t\t\n",
    "# \t\t\tif not knownFlag:\n",
    "# \t\t\t\tsys.stderr.write('First argument must be either leq or eq.\\n')\n",
    "# \t\t\t\tsys.exit(1)\n",
    "\t\t\t\n",
    "# \t\t\tsys.stdout.write(result)\n",
    "# \t\t\tsys.exit(0)\n",
    "\n",
    "# \t\texcept ValueError:\n",
    "# \t\t\tsys.stderr.write('Second argument must be an integer.\\n')\n",
    "# \t\t\tsys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recCheckEmbedding(pattern, transaction, mapping):\n",
    "    # check if we are in a leaf vertex in both pattern and transaction\n",
    "    if 'prediction' in pattern.keys() and 'prediction' in transaction.keys():\n",
    "        mapping[pattern['id']] = transaction['id']\n",
    "        return True\n",
    "    \n",
    "    # check if we are in a split vertex in both pattern and transaction\n",
    "    if 'feature' in pattern.keys() and 'feature' in transaction.keys():\n",
    "\n",
    "        # check if split features match\n",
    "        if pattern['feature'] == transaction['feature']:\n",
    "            \n",
    "            foundLeft = True\n",
    "            foundRight = True\n",
    "            if 'leftChild' in pattern.keys():\n",
    "                if 'leftChild' in transaction.keys():\n",
    "                    foundLeft = recCheckEmbedding(pattern['leftChild'], transaction['leftChild'], mapping)    \n",
    "                else:\n",
    "                    foundLeft = False\n",
    "                \n",
    "            if 'rightChild' in pattern.keys():\n",
    "                if 'rightChild' in transaction.keys():\n",
    "                    foundRight = recCheckEmbedding(pattern['rightChild'], transaction['rightChild'], mapping)\n",
    "                else:\n",
    "                    foundRight = False\n",
    "                \n",
    "            if foundLeft and foundRight:\n",
    "                mapping[pattern['id']] = transaction['id']\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "    # if we are in the mixed case split vertex vs. leaf vertex then we cannot map the vertices on each other\n",
    "    return False\n",
    "\n",
    "\n",
    "def checkForEmbedding(pattern, transaction):\n",
    "    '''For two given root vertices, check whether pattern is a rooted \n",
    "    subtree of transaction such that the roots map to each other \n",
    "    and return a mapping id->id if so, o/w None'''\n",
    "    \n",
    "    mapping = dict()\n",
    "    if recCheckEmbedding(pattern, transaction, mapping):\n",
    "        return mapping\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def findAllEmbeddings(pattern, patternid, transaction):\n",
    "    '''Find all embeddings of pattern into transaction and store them in the transaction\n",
    "    at the positions where the root vertex of the pattern maps to.\n",
    "    This method expects to be called after initTransactionTreeForEmbeddingStorage()'''\n",
    "    if 'feature' in transaction.keys():\n",
    "        if 'leftChild' in transaction.keys():\n",
    "            findAllEmbeddings(pattern, patternid, transaction['leftChild'])\n",
    "        if 'rightChild' in transaction.keys():\n",
    "            findAllEmbeddings(pattern, patternid, transaction['rightChild'])\n",
    "    \n",
    "    mapping = checkForEmbedding(pattern, transaction)\n",
    "    if mapping != None:\n",
    "        transaction['patterns'].append((patternid, mapping))\n",
    "        print(transaction['patterns'])\n",
    "\n",
    "def initTransactionTreeForEmbeddingStorage(transaction):\n",
    "    '''We want to be able to store all matching patterns and their embeddings at the\n",
    "    the vertices, where the root of the pattern maps to. Hence, we init some fields \n",
    "    in the transaction decision tree.'''\n",
    "\n",
    "    if 'feature' in transaction.keys():\n",
    "        if 'leftChild' in transaction.keys():\n",
    "            initTransactionTreeForEmbeddingStorage(transaction['leftChild'])\n",
    "        if 'rightChild' in transaction.keys():\n",
    "            initTransactionTreeForEmbeddingStorage(transaction['rightChild'])\n",
    "    \n",
    "    transaction['patterns'] = list()\n",
    "\n",
    "\n",
    "def loadAndProcess(patternInput, transactionInput, transactionOutput):\n",
    "    transactions = json.load(transactionInput)\n",
    "    patterns = json.load(patternInput)\n",
    "    \n",
    "    for transaction in transactions:\n",
    "        initTransactionTreeForEmbeddingStorage(transaction)\n",
    "        \n",
    "    for transaction in transactions:\n",
    "        for pattern in patterns:\n",
    "            findAllEmbeddings(pattern['pattern'], pattern['patternid'], transaction)\n",
    "            \n",
    "\n",
    "            \n",
    "    json.dump(transactions, transactionOutput)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternIn = open('forests/rootedFrequentTrees/adult/WithLeafEdges/RF_10_t10.patterns', 'r')\n",
    "patternJson = parseCStringFileFixedSizePatterns(patternIn, 4)\n",
    "patternIn.close()\n",
    "\n",
    "patternjsondump = open('testPatterns.json', 'w')\n",
    "patternjsondump.write(patternJson)\n",
    "patternjsondump.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(337, {2: 141, 3: 142, 1: 140, 0: 139})]\n",
      "[(337, {2: 224, 3: 225, 1: 223, 0: 222})]\n",
      "[(337, {2: 141, 3: 142, 1: 140, 0: 139}), (336, {2: 141, 1: 140, 3: 143, 0: 139})]\n",
      "[(338, {2: 369, 3: 370, 1: 368, 0: 358})]\n",
      "[(335, {2: 364, 3: 365, 1: 363, 0: 361})]\n",
      "[(328, {2: 446, 3: 447, 1: 445, 0: 437})]\n",
      "[(337, {2: 177, 3: 178, 1: 176, 0: 175})]\n",
      "[(337, {2: 177, 3: 178, 1: 176, 0: 175}), (336, {2: 177, 1: 176, 3: 179, 0: 175})]\n",
      "[(327, {1: 570, 3: 573, 2: 571, 0: 569})]\n",
      "[(328, {2: 468, 3: 469, 1: 467, 0: 463})]\n",
      "[(327, {1: 570, 3: 573, 2: 571, 0: 569}), (328, {2: 572, 3: 573, 1: 571, 0: 569})]\n",
      "[(326, {2: 618, 3: 619, 1: 617, 0: 609})]\n",
      "[(330, {2: 273, 3: 274, 1: 272, 0: 268})]\n",
      "[(330, {2: 335, 3: 336, 1: 334, 0: 332})]\n",
      "[(329, {2: 34, 3: 35, 1: 33, 0: 29})]\n",
      "[(329, {2: 97, 3: 98, 1: 96, 0: 92})]\n",
      "[(327, {1: 436, 3: 441, 2: 437, 0: 435})]\n",
      "[(337, {2: 514, 3: 515, 1: 513, 0: 512})]\n",
      "[(337, {2: 514, 3: 515, 1: 513, 0: 512}), (336, {2: 514, 1: 513, 3: 516, 0: 512})]\n",
      "[(335, {2: 414, 3: 415, 1: 413, 0: 409})]\n",
      "[(334, {2: 534, 3: 535, 1: 533, 0: 531})]\n",
      "[(333, {2: 394, 3: 395, 1: 393, 0: 392})]\n",
      "[(333, {2: 609, 3: 610, 1: 608, 0: 607})]\n",
      "[(333, {2: 394, 3: 395, 1: 393, 0: 392}), (332, {2: 394, 1: 393, 3: 396, 0: 392})]\n",
      "[(331, {2: 301, 3: 302, 1: 300, 0: 298})]\n",
      "[(329, {2: 329, 3: 330, 1: 328, 0: 324})]\n",
      "[(329, {2: 384, 3: 385, 1: 383, 0: 381})]\n",
      "[(326, {2: 62, 3: 63, 1: 61, 0: 59})]\n",
      "[(326, {2: 509, 3: 510, 1: 508, 0: 506})]\n",
      "[(339, {2: 292, 1: 291, 3: 294, 0: 290})]\n",
      "[(337, {2: 200, 3: 201, 1: 199, 0: 198})]\n",
      "[(338, {2: 324, 3: 325, 1: 323, 0: 317})]\n",
      "[(334, {2: 56, 3: 57, 1: 55, 0: 53})]\n",
      "[(331, {2: 36, 3: 37, 1: 35, 0: 33})]\n",
      "[(329, {2: 85, 3: 86, 1: 84, 0: 82})]\n",
      "[(327, {1: 108, 3: 111, 2: 109, 0: 107})]\n",
      "[(327, {1: 108, 3: 111, 2: 109, 0: 107}), (328, {2: 110, 3: 111, 1: 109, 0: 107})]\n",
      "[(326, {2: 314, 3: 315, 1: 313, 0: 311})]\n",
      "[(339, {2: 213, 1: 212, 3: 215, 0: 211})]\n",
      "[(337, {2: 371, 3: 372, 1: 370, 0: 369})]\n",
      "[(337, {2: 371, 3: 372, 1: 370, 0: 369}), (338, {2: 374, 3: 375, 1: 373, 0: 369})]\n",
      "[(335, {2: 36, 3: 37, 1: 35, 0: 31})]\n",
      "[(334, {2: 488, 3: 489, 1: 487, 0: 485})]\n",
      "[(333, {2: 192, 3: 193, 1: 191, 0: 190})]\n",
      "[(333, {2: 192, 3: 193, 1: 191, 0: 190}), (332, {2: 192, 1: 191, 3: 194, 0: 190})]\n",
      "[(330, {2: 171, 3: 172, 1: 170, 0: 166})]\n",
      "[(327, {1: 446, 3: 449, 2: 447, 0: 445})]\n",
      "[(327, {1: 502, 3: 505, 2: 503, 0: 501})]\n",
      "[(328, {2: 358, 3: 359, 1: 357, 0: 353})]\n",
      "[(327, {1: 446, 3: 449, 2: 447, 0: 445}), (328, {2: 448, 3: 449, 1: 447, 0: 445})]\n",
      "[(327, {1: 502, 3: 505, 2: 503, 0: 501}), (328, {2: 504, 3: 505, 1: 503, 0: 501})]\n",
      "[(326, {2: 377, 3: 378, 1: 376, 0: 368})]\n",
      "[(337, {2: 278, 3: 279, 1: 277, 0: 276})]\n",
      "[(337, {2: 278, 3: 279, 1: 277, 0: 276}), (338, {2: 281, 3: 282, 1: 280, 0: 276})]\n",
      "[(334, {2: 222, 3: 223, 1: 221, 0: 217})]\n",
      "[(333, {2: 735, 3: 736, 1: 734, 0: 733})]\n",
      "[(333, {2: 735, 3: 736, 1: 734, 0: 733}), (332, {2: 735, 1: 734, 3: 737, 0: 733})]\n",
      "[(330, {2: 790, 3: 791, 1: 789, 0: 787})]\n",
      "[(329, {2: 598, 3: 599, 1: 597, 0: 595})]\n",
      "[(327, {1: 329, 3: 332, 2: 330, 0: 328})]\n",
      "[(327, {1: 329, 3: 332, 2: 330, 0: 328}), (328, {2: 331, 3: 332, 1: 330, 0: 328})]\n",
      "[(326, {2: 68, 3: 69, 1: 67, 0: 57})]\n",
      "[(339, {2: 555, 1: 554, 3: 557, 0: 553})]\n",
      "[(338, {2: 411, 3: 412, 1: 410, 0: 406})]\n",
      "[(338, {2: 546, 3: 547, 1: 545, 0: 543})]\n",
      "[(338, {2: 661, 3: 662, 1: 660, 0: 658})]\n",
      "[(335, {2: 673, 3: 674, 1: 672, 0: 670})]\n",
      "[(333, {2: 372, 3: 373, 1: 371, 0: 370})]\n",
      "[(333, {2: 372, 3: 373, 1: 371, 0: 370}), (332, {2: 372, 1: 371, 3: 374, 0: 370})]\n",
      "[(330, {2: 398, 3: 399, 1: 397, 0: 395})]\n",
      "[(339, {2: 120, 1: 119, 3: 126, 0: 118})]\n",
      "[(333, {2: 599, 3: 600, 1: 598, 0: 597})]\n",
      "[(331, {2: 573, 3: 574, 1: 572, 0: 568})]\n",
      "[(329, {2: 115, 3: 116, 1: 114, 0: 112})]\n",
      "[(327, {1: 303, 3: 306, 2: 304, 0: 302})]\n",
      "[(327, {1: 477, 3: 480, 2: 478, 0: 476})]\n",
      "[(327, {1: 303, 3: 306, 2: 304, 0: 302}), (328, {2: 305, 3: 306, 1: 304, 0: 302})]\n",
      "[(327, {1: 477, 3: 480, 2: 478, 0: 476}), (328, {2: 479, 3: 480, 1: 478, 0: 476})]\n",
      "[(337, {2: 533, 3: 534, 1: 532, 0: 531})]\n",
      "[(337, {2: 533, 3: 534, 1: 532, 0: 531}), (338, {2: 536, 3: 537, 1: 535, 0: 531})]\n",
      "[(334, {2: 514, 3: 515, 1: 513, 0: 511})]\n",
      "[(333, {2: 716, 3: 717, 1: 715, 0: 714})]\n",
      "[(333, {2: 716, 3: 717, 1: 715, 0: 714}), (332, {2: 716, 1: 715, 3: 718, 0: 714})]\n",
      "[(331, {2: 304, 3: 305, 1: 303, 0: 283})]\n",
      "[(330, {2: 33, 3: 34, 1: 32, 0: 28})]\n",
      "[(329, {2: 99, 3: 100, 1: 98, 0: 94})]\n",
      "[(326, {2: 219, 3: 220, 1: 218, 0: 216})]\n",
      "[(326, {2: 527, 3: 528, 1: 526, 0: 522})]\n",
      "[(339, {2: 207, 1: 206, 3: 211, 0: 205})]\n",
      "[(339, {2: 622, 1: 621, 3: 628, 0: 620})]\n",
      "[(337, {2: 412, 3: 413, 1: 411, 0: 410})]\n",
      "[(335, {2: 522, 3: 523, 1: 521, 0: 519})]\n",
      "[(334, {2: 44, 3: 45, 1: 43, 0: 39})]\n",
      "[(330, {2: 336, 3: 337, 1: 335, 0: 331})]\n",
      "[(330, {2: 509, 3: 510, 1: 508, 0: 506})]\n",
      "[(329, {2: 146, 3: 147, 1: 145, 0: 141})]\n",
      "[(329, {2: 201, 3: 202, 1: 200, 0: 198})]\n",
      "[(336, {2: 336, 1: 335, 3: 340, 0: 334})]\n",
      "[(338, {2: 521, 3: 522, 1: 520, 0: 518})]\n",
      "[(331, {2: 484, 3: 485, 1: 483, 0: 477})]\n",
      "[(331, {2: 514, 3: 515, 1: 513, 0: 507})]\n",
      "[(339, {2: 369, 1: 368, 3: 371, 0: 367})]\n",
      "[(337, {2: 398, 3: 399, 1: 397, 0: 396})]\n",
      "[(337, {2: 398, 3: 399, 1: 397, 0: 396}), (336, {2: 398, 1: 397, 3: 400, 0: 396})]\n",
      "[(338, {2: 591, 3: 592, 1: 590, 0: 586})]\n",
      "[(335, {2: 143, 3: 144, 1: 142, 0: 136})]\n",
      "[(335, {2: 527, 3: 528, 1: 526, 0: 522})]\n",
      "[(329, {2: 485, 3: 486, 1: 484, 0: 482})]\n",
      "[(326, {2: 91, 3: 92, 1: 90, 0: 88})]\n",
      "[(326, {2: 532, 3: 533, 1: 531, 0: 529})]\n",
      "[(326, {2: 551, 3: 552, 1: 550, 0: 548})]\n",
      "[(339, {2: 446, 1: 445, 3: 448, 0: 444})]\n",
      "[(333, {2: 211, 3: 212, 1: 210, 0: 209})]\n",
      "[(333, {2: 529, 3: 530, 1: 528, 0: 527})]\n",
      "[(331, {2: 191, 3: 192, 1: 190, 0: 186})]\n",
      "[(331, {2: 281, 3: 282, 1: 280, 0: 278})]\n",
      "[(328, {2: 366, 3: 367, 1: 365, 0: 361})]\n",
      "[(326, {2: 249, 3: 250, 1: 248, 0: 244})]\n",
      "[(326, {2: 454, 3: 455, 1: 453, 0: 449})]\n",
      "[(326, {2: 461, 3: 462, 1: 460, 0: 458})]\n",
      "[(339, {2: 598, 1: 597, 3: 600, 0: 596})]\n",
      "[(337, {2: 289, 3: 290, 1: 288, 0: 287})]\n",
      "[(337, {2: 666, 3: 667, 1: 665, 0: 664})]\n",
      "[(337, {2: 666, 3: 667, 1: 665, 0: 664}), (336, {2: 666, 1: 665, 3: 668, 0: 664})]\n",
      "[(333, {2: 324, 3: 325, 1: 323, 0: 322})]\n",
      "[(333, {2: 580, 3: 581, 1: 579, 0: 578})]\n",
      "[(332, {2: 663, 1: 662, 3: 669, 0: 661})]\n",
      "[(333, {2: 324, 3: 325, 1: 323, 0: 322}), (331, {2: 327, 3: 328, 1: 326, 0: 322})]\n",
      "[(331, {2: 633, 3: 634, 1: 632, 0: 628})]\n",
      "[(327, {1: 660, 3: 669, 2: 661, 0: 659})]\n",
      "[(326, {2: 534, 3: 535, 1: 533, 0: 529})]\n",
      "[(337, {2: 244, 3: 245, 1: 243, 0: 242})]\n",
      "[(338, {2: 79, 3: 80, 1: 78, 0: 74})]\n",
      "[(338, {2: 401, 3: 402, 1: 400, 0: 398})]\n",
      "[(335, {2: 54, 3: 55, 1: 53, 0: 49})]\n",
      "[(335, {2: 64, 3: 65, 1: 63, 0: 59})]\n",
      "[(335, {2: 210, 3: 211, 1: 209, 0: 207})]\n",
      "[(334, {2: 515, 3: 516, 1: 514, 0: 510})]\n",
      "[(333, {2: 330, 3: 331, 1: 329, 0: 328})]\n",
      "[(331, {2: 411, 3: 412, 1: 410, 0: 406})]\n",
      "[(330, {2: 588, 3: 589, 1: 587, 0: 585})]\n",
      "[(327, {1: 288, 3: 291, 2: 289, 0: 287})]\n",
      "[(327, {1: 318, 3: 321, 2: 319, 0: 317})]\n",
      "[(327, {1: 288, 3: 291, 2: 289, 0: 287}), (328, {2: 290, 3: 291, 1: 289, 0: 287})]\n",
      "[(327, {1: 318, 3: 321, 2: 319, 0: 317}), (328, {2: 320, 3: 321, 1: 319, 0: 317})]\n",
      "[(339, {2: 266, 1: 265, 3: 268, 0: 264})]\n",
      "[(338, {2: 433, 3: 434, 1: 432, 0: 428})]\n",
      "[(334, {2: 405, 3: 406, 1: 404, 0: 400})]\n",
      "[(333, {2: 487, 3: 488, 1: 486, 0: 485})]\n",
      "[(333, {2: 487, 3: 488, 1: 486, 0: 485}), (332, {2: 487, 1: 486, 3: 489, 0: 485})]\n",
      "[(330, {2: 334, 3: 335, 1: 333, 0: 329})]\n",
      "[(338, {2: 178, 3: 179, 1: 177, 0: 175})]\n",
      "[(333, {2: 483, 3: 484, 1: 482, 0: 481})]\n",
      "[(331, {2: 389, 3: 390, 1: 388, 0: 386})]\n",
      "[(333, {2: 483, 3: 484, 1: 482, 0: 481}), (331, {2: 486, 3: 487, 1: 485, 0: 481})]\n",
      "[(327, {1: 225, 3: 232, 2: 226, 0: 224})]\n",
      "[(326, {2: 435, 3: 436, 1: 434, 0: 432})]\n",
      "[(339, {2: 165, 1: 164, 3: 169, 0: 163})]\n",
      "[(337, {2: 281, 3: 282, 1: 280, 0: 279})]\n",
      "[(337, {2: 281, 3: 282, 1: 280, 0: 279}), (336, {2: 281, 1: 280, 3: 283, 0: 279})]\n",
      "[(338, {2: 20, 3: 21, 1: 19, 0: 15})]\n",
      "[(335, {2: 317, 3: 318, 1: 316, 0: 312})]\n",
      "[(333, {2: 468, 3: 469, 1: 467, 0: 466})]\n",
      "[(332, {2: 521, 1: 520, 3: 525, 0: 519})]\n",
      "[(329, {2: 130, 3: 131, 1: 129, 0: 127})]\n",
      "[(326, {2: 401, 3: 402, 1: 400, 0: 396})]\n",
      "[(339, {2: 273, 1: 272, 3: 287, 0: 271})]\n",
      "[(337, {2: 350, 3: 351, 1: 349, 0: 348})]\n",
      "[(337, {2: 350, 3: 351, 1: 349, 0: 348}), (336, {2: 350, 1: 349, 3: 352, 0: 348})]\n",
      "[(335, {2: 324, 3: 325, 1: 323, 0: 319})]\n",
      "[(334, {2: 127, 3: 128, 1: 126, 0: 122})]\n",
      "[(334, {2: 302, 3: 303, 1: 301, 0: 299})]\n",
      "[(333, {2: 479, 3: 480, 1: 478, 0: 477})]\n",
      "[(333, {2: 479, 3: 480, 1: 478, 0: 477}), (332, {2: 479, 1: 478, 3: 481, 0: 477})]\n",
      "[(327, {1: 146, 3: 153, 2: 147, 0: 145})]\n",
      "[(326, {2: 172, 3: 173, 1: 171, 0: 167})]\n",
      "[(326, {2: 234, 3: 235, 1: 233, 0: 229})]\n",
      "[(326, {2: 263, 3: 264, 1: 262, 0: 258})]\n",
      "[(326, {2: 400, 3: 401, 1: 399, 0: 397})]\n",
      "[(326, {2: 444, 3: 445, 1: 443, 0: 441})]\n",
      "[(335, {2: 193, 3: 194, 1: 192, 0: 190})]\n",
      "[(330, {2: 440, 3: 441, 1: 439, 0: 435})]\n",
      "[(329, {2: 174, 3: 175, 1: 173, 0: 169})]\n",
      "[(338, {2: 175, 3: 176, 1: 174, 0: 170})]\n",
      "[(330, {2: 371, 3: 372, 1: 370, 0: 366})]\n",
      "[(338, {2: 622, 3: 623, 1: 621, 0: 619})]\n",
      "[(334, {2: 136, 3: 137, 1: 135, 0: 131})]\n",
      "[(329, {2: 240, 3: 241, 1: 239, 0: 237})]\n",
      "[(326, {2: 427, 3: 428, 1: 426, 0: 424})]\n",
      "[(339, {2: 144, 1: 143, 3: 146, 0: 142})]\n",
      "[(336, {2: 341, 1: 340, 3: 345, 0: 339})]\n",
      "[(338, {2: 47, 3: 48, 1: 46, 0: 44})]\n",
      "[(333, {2: 173, 3: 174, 1: 172, 0: 171})]\n",
      "[(328, {2: 152, 3: 153, 1: 151, 0: 147})]\n",
      "[(339, {2: 209, 1: 208, 3: 211, 0: 207})]\n",
      "[(339, {2: 468, 1: 467, 3: 472, 0: 466})]\n",
      "[(337, {2: 131, 3: 132, 1: 130, 0: 129})]\n",
      "[(337, {2: 557, 3: 558, 1: 556, 0: 555})]\n",
      "[(337, {2: 557, 3: 558, 1: 556, 0: 555}), (336, {2: 557, 1: 556, 3: 559, 0: 555})]\n",
      "[(335, {2: 235, 3: 236, 1: 234, 0: 232})]\n",
      "[(335, {2: 508, 3: 509, 1: 507, 0: 503})]\n",
      "[(334, {2: 418, 3: 419, 1: 417, 0: 413})]\n",
      "[(333, {2: 138, 3: 139, 1: 137, 0: 136})]\n",
      "[(333, {2: 253, 3: 254, 1: 252, 0: 251})]\n",
      "[(333, {2: 438, 3: 439, 1: 437, 0: 436})]\n",
      "[(333, {2: 253, 3: 254, 1: 252, 0: 251}), (332, {2: 253, 1: 252, 3: 255, 0: 251})]\n",
      "[(331, {2: 494, 3: 495, 1: 493, 0: 491})]\n",
      "[(328, {2: 388, 3: 389, 1: 387, 0: 375})]\n",
      "[(328, {2: 594, 3: 595, 1: 593, 0: 589})]\n"
     ]
    }
   ],
   "source": [
    "transactionInFile = open('forests/adult/text/RF_10.json', 'r')\n",
    "patternInFile = open('testPatterns.json', 'r')\n",
    "\n",
    "transactionOutFile = open('testTransactions.json', 'w')\n",
    "\n",
    "loadAndProcess(patternInFile, transactionInFile, transactionOutFile)\n",
    "\n",
    "transactionInFile.close()\n",
    "patternInFile.close()\n",
    "transactionOutFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactionOutFile = open('testTransactions.json', 'r')\n",
    "result = json.load(transactionOutFile)\n",
    "transactionOutFile.close()\n",
    "\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have now the tools to transform the data and compute all embeddings of frequent patterns explicitly. \n",
    "This will be soon put in a nice script that is usable from the command line.\n",
    "\n",
    "As a fist quick glance, it seems that the patterns that were identified as frequent mostly have only one embedding per random decision tree. \n",
    "But this needs to be investigated more closely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
