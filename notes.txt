private notes of Manuel and Markus

--------------------
Was haben wir gemacht?
- pruning.py erstellt f체r das prunen der Random Forests (json -> json)
- trainForest.py im arch-forest Ordner abge채ndert. Fuer jeden Datensatz den Befehl "X.dump("FeatureVectors.dat")" in die trainForest Datei eingefuegt. Dadurch entstehen auch neue Dateien (FeatureVectors.dat) fuer jeden Datensatz im arch-forest Order.
- '2 Initial Rooted Frequent Subtree Mining -- With Split Values'.ipynb erstellt fuer das frequent-subtree-mining. Nur fuer threshold 2! 


--------------------
Todo:
- thresholds 3 bis 25 aus den threshold 2 Dateien erstellen
- Auswertung der Ergebnisse
- andere pruning-kriterien ausprobieren (path-changeable rate sigma)


--------------------
Programme ueber ssh ausfuehren:
1. jupyter nbconvert --ExecutePreprocessor.timeout=-1 --to markdown --execute '0 Pruning Distinct Branching Conditions.ipynb'
2. jupyter nbconvert --ExecutePreprocessor.timeout=-1 --allow-errors --to markdown --execute '2 Initial Rooted Frequent Subtree Mining -- With Split Values -- without leaves -- after pruning.ipynb'

Bemerkung: Notebook 1 (data Conversion) wird nicht gebraucht, bereits in unserem Notebook 2 enthalten


--------------------
moeglicherweise interessante Dateien im Repository (f체r die Auswertung der Ergebnisse):
- DSF_RF.ipynb -> DSF Random Forest (folder) 
- RF_all_accuracy.csv in forests/rootedFrequentTrees/adult
- LearningAlgo.ipynb

f체r Auswertung nur einen Datensatz benutzen (adult)?

!!! im Notebook Program.ipynb werden evtl die Dateien erstellt die in LearningAlgo.ipynb benutzt werden !!!
